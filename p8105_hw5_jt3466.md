p8105_hw5_jt3466
================
Johnstone Tcheou
2024-11-05

# Question 1

``` r
bday_sim <- function(n) {
  
  bdays <- sample(1:365, size = n, replace = TRUE)
  
  duplicate = length(unique(bdays)) < n
  
  return(duplicate)
  
}

bday_2_50 <- 
  expand_grid(
    n = 2:50,
    iter = 1:10000
  ) |> 
  mutate(
    res = map_lgl(n, bday_sim) 
  ) |> 
  group_by(n) |> 
  summarize(prob = mean(res))

bday_2_50 |> 
  ggplot(aes(x = n, y = prob)) + 
  geom_line()
```

![](p8105_hw5_jt3466_files/figure-gfm/q1-1.png)<!-- --> Based on the
graph, we see that the probability of having at least 2 people with the
same birthday is approximately logarithmic. Around a group size of 23,
the probability of having 2 or more with the same birthday is around
50%, which gradually increases until plateauing near 100% with around 50
people in the group. The slope of probability per group size appears to
be the highest from a group size of 10 to 30.

# Question 2

``` r
pop_size <- 30
pop_sd <- 5
alpha <- 0.05

# rnorm(mean = 0, n = pop_size, sd = pop_sd) |> 
#   t.test(mu = 0, alternative = "two.sided")
# 
# test <-
#   expand_grid(
#     i = 5,
#     pop_mean = c(0:6)
#   ) |> 
#   mutate(
#     data = map(pop_mean, \(x) rnorm(n = pop_size, mean = x, sd = pop_sd)),
#     ttest = map2(data, pop_mean, \(x, y) t.test(x = x, mu = y, alternative = "two.sided")),
#     results = map(ttest, broom::tidy)
#   ) |> 
#   unnest(cols = results)

# test_sim_data <- 
#   expand_grid(
#     i = 1:5000,
#     pop_mean = c(0:6)
#   ) |>
#   mutate(
#     data = map(pop_mean, \(x) rnorm(n = pop_size, mean = x, sd = pop_sd)),
#     ttest = map2(data, pop_mean, \(x, y) t.test(x = x, mu = y, alternative = "two.sided")),
#     results = map(ttest, broom::tidy)
#   ) |> 
#   unnest(cols = results) 

sim_data <- 
  expand_grid(
    i = 1:5000,
    pop_mean = c(0:6)
  ) |>
  mutate(
    data = map(pop_mean, \(x) rnorm(n = pop_size, mean = x, sd = pop_sd)),
    ttest = map(data, \(x) t.test(x = x, mu = 0, alternative = "two.sided")),
    results = map(ttest, broom::tidy)
  ) |> 
  unnest(cols = results) 
  
# sim_res_data <- 
#   sim_data |> 
#   select(pop_mean, estimate, p.value) |> 
#   rename(estimated_mean = estimate) |> 
#   arrange(pop_mean) |> 
#   group_by(pop_mean) |> 
#   mutate(
#     significant = case_when(
#       p.value < 0.05 ~ 1,
#       p.value > 0.05 ~ 0,
#       .default = NA
#     ), 
#     prptn_sig = mean(significant)
#   ) 
# 
# sim_res_data |> 
#   ggplot(aes(x = pop_mean, y = prptn_sig)) + 
#   geom_line()

# sim_data <-
#   expand_grid(
#     i = 1:5000,
#     pop_mean = c(0:6)
#   ) |>
#   mutate(
#     data = map(pop_mean, \(x) rnorm(n = pop_size, mean = x, sd = pop_sd)),
#     ttest = map2(data, pop_mean, \(x, y) t.test(x = x, mu = y, alternative = "two.sided")),
#     results = map(ttest, broom::tidy)
#   ) |> 
#   unnest(cols = results) 
# 
# # sim_data |> 
# #   mutate(
# #     mean = map_dbl(data, mean),
# #     sd = map_dbl(data, sd)
# #   ) |> 
# #   select(mean, pop_mean, estimate, sd, everything())
# 
#   
# sim_res_data <- 
#   sim_data |> 
#   select(pop_mean, estimate, p.value) |> 
#   rename(estimated_mean = estimate) |> 
#   arrange(pop_mean) |> 
#   group_by(pop_mean) |> 
#   mutate(
#     significant = case_when(
#       p.value < 0.05 ~ 1,
#       p.value > 0.05 ~ 0,
#       .default = NA
#     ), 
#     prptn_sig = mean(significant)
#   ) 
# 
# sim_res_data |> 
#   ggplot(aes(x = pop_mean, y = prptn_sig)) + 
#   geom_line()
```

``` r
sim_res_data <- 
  sim_data |> 
  select(pop_mean, estimate, p.value) |> 
  rename(estimated_mean = estimate) |> 
  arrange(pop_mean) |> 
  group_by(pop_mean) |> 
  mutate(
    significant = case_when(
      p.value < 0.05 ~ 1,
      p.value > 0.05 ~ 0,
      .default = NA
    ), 
    prptn_sig = mean(significant)
  ) 

sim_res_data |> 
  ggplot(aes(x = pop_mean, y = prptn_sig)) + 
  geom_line() +
  labs(
      x = "True population mean (mu)",
      y = "Proportion of significantly different estimated means \n(power of test)"
    ) 
```

![](p8105_hw5_jt3466_files/figure-gfm/mu%20vs%20proportion%20significant-1.png)<!-- -->

Based on the graph, we see that as population mean increases, the
proportion of tests that are significant increases. It the greatest
increase in proportion significant is between 1 to 3, where it starts to
taper off and plateau close to 100% by a population mean of 4.

As such, this indicates that as population mean increases, it is getting
further from the mu we are testing of 0, so effect size is increasing,
and the power to detect a significant difference increases as the effect
size increases.

``` r
avg_mu_hats <-
  sim_res_data |> 
  group_by(pop_mean) |> 
  mutate(
    avg_estimated_mean = mean(estimated_mean) 
  )

sig_avg_mu_hats <- 
  sim_res_data |> 
  filter(significant == 1) |> 
  group_by(pop_mean) |> 
  mutate(
    avg_estimated_mean = mean(estimated_mean) 
  )

avg_mu_hats |> 
  filter(pop_mean == 1) |> 
  summarize(est_mean = mean(estimated_mean)) |> 
  pull(est_mean)
```

    ## [1] 1.00145

``` r
sig_avg_mu_hats |> 
  filter(pop_mean == 1) |> 
  summarize(est_mean = mean(estimated_mean)) |> 
  pull(est_mean)
```

    ## [1] 2.254885

``` r
avg_mu_hats |> 
  ggplot(aes(x = pop_mean, y = estimated_mean)) +
  geom_point() + 
  geom_line(aes(x = pop_mean, y = avg_estimated_mean)) + 
  geom_point(data = sig_avg_mu_hats, 
             aes(x = pop_mean, y = estimated_mean, color = "red"),
             alpha = 0.25) + 
  geom_line(data = sig_avg_mu_hats, 
            aes(x = pop_mean, y = avg_estimated_mean, color = "red"), 
            linetype = "dashed") + 
  scale_y_continuous(
    breaks = seq(-3, 10, by = 1)
  ) +
  scale_x_continuous(
    breaks = seq(0, 6, by = 1)
  ) +
  labs(
      x = "True population mean (mu)",
      y = "Average of estimated means (average of mu hat)"
    ) +
  guides(
    color = "none"
  )
```

![](p8105_hw5_jt3466_files/figure-gfm/q2%20avg%20estimated%20mu%20vs%20true-1.png)<!-- -->

The plot shows that as the true population mean increases, the average
estimated mean also increases. The black points are all the estimated
means for a given population mean, while the red points are the
estimated means with a significant difference from the population mean.
This is in line with the last graph, where a larger proportion of points
are significant as the true population mean is higher (indicating larger
effect size with increasing true population mean)

As expected, the average estimated mean for all estimates of the same
population mean is approximately the population mean. However, for the
sample average estimated means for estimates significantly different
from the null, the trend is largely similar except when the population
mean is 1 and to a lesser extent, 2. When the population mean is 1, the
average estimated mean among significant estimates is 2.2548846 instead
of 1. When the population mean is 2, the average estimated mean among
significant estimates is 2.2548846.

# Question 3

``` r
wp_data <- 
  read_csv(
    file = "https://raw.githubusercontent.com/washingtonpost/data-homicides/refs/heads/master/homicide-data.csv"
  ) 
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
summary(wp_data)
```

    ##      uid            reported_date       victim_last        victim_first      
    ##  Length:52179       Min.   : 20070101   Length:52179       Length:52179      
    ##  Class :character   1st Qu.: 20100318   Class :character   Class :character  
    ##  Mode  :character   Median : 20121216   Mode  :character   Mode  :character  
    ##                     Mean   : 20130899                                        
    ##                     3rd Qu.: 20150911                                        
    ##                     Max.   :201511105                                        
    ##                                                                              
    ##  victim_race         victim_age         victim_sex            city          
    ##  Length:52179       Length:52179       Length:52179       Length:52179      
    ##  Class :character   Class :character   Class :character   Class :character  
    ##  Mode  :character   Mode  :character   Mode  :character   Mode  :character  
    ##                                                                             
    ##                                                                             
    ##                                                                             
    ##                                                                             
    ##     state                lat             lon          disposition       
    ##  Length:52179       Min.   :25.73   Min.   :-122.51   Length:52179      
    ##  Class :character   1st Qu.:33.77   1st Qu.: -96.00   Class :character  
    ##  Mode  :character   Median :38.52   Median : -87.71   Mode  :character  
    ##                     Mean   :37.03   Mean   : -91.47                     
    ##                     3rd Qu.:40.03   3rd Qu.: -81.76                     
    ##                     Max.   :45.05   Max.   : -71.01                     
    ##                     NA's   :60      NA's   :60

The raw dataset is 52179 observations long and 12 variables wide for 50
different cities. The key variables include `victim_first` and
`victim_last` to provide the victim’s name, as well as victim
demographic info like `victim_age`, `victim_sex`, and `victim_race`, as
well as the `city` and `state` (with longitudinal data) and the
`reported_date` of the homicide. The `reported_date` variable was
imported as a numeric type, not as a library type, and there is an
observation with a date of 2.015111^{8}, which is not sensical.

``` r
wp_data <- 
  read_csv(
    file = "https://raw.githubusercontent.com/washingtonpost/data-homicides/refs/heads/master/homicide-data.csv"
  ) |> 
  mutate(
    reported_date = case_match(
      reported_date,
      201511105 ~ 20151105,
      201511018 ~ 20151018,
      .default = reported_date
    ),
    reported_date = ymd(reported_date),
    city_state = paste(city, state, sep = ", ")
  )
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

After changing that observation, there is another date with `201511018`,
so it appears these all have an extra 1 before the month. These two
observations are changed accordingly. Now that all `reported_date`s are
in the format `yyyymmdd`, the `ymd` function from the `lubridate`
package is used to convert these to dates. A variable `city_state` is
created from concatenating the `city` and `state` variables. The
variable names are already tidy, so `clean_names` from the `janitor`
package is not needed.

``` r
wp_prop <-
  wp_data |> 
  group_by(city_state) |> 
  summarize(total_homicides = n(),
            open_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))) |> 
  mutate(
    prop_unsolved = open_homicides/total_homicides
  )

wp_prop
```

    ## # A tibble: 51 × 4
    ##    city_state      total_homicides open_homicides prop_unsolved
    ##    <chr>                     <int>          <int>         <dbl>
    ##  1 Albuquerque, NM             378            146         0.386
    ##  2 Atlanta, GA                 973            373         0.383
    ##  3 Baltimore, MD              2827           1825         0.646
    ##  4 Baton Rouge, LA             424            196         0.462
    ##  5 Birmingham, AL              800            347         0.434
    ##  6 Boston, MA                  614            310         0.505
    ##  7 Buffalo, NY                 521            319         0.612
    ##  8 Charlotte, NC               687            206         0.300
    ##  9 Chicago, IL                5535           4073         0.736
    ## 10 Cincinnati, OH              694            309         0.445
    ## # ℹ 41 more rows

``` r
baltimore_prop <- 
  wp_prop |> 
  filter(city_state == "Baltimore, MD") |> 
  mutate(
    prop_test = map(open_homicides, prop.test, n = total_homicides),
    prop_test_tidy = map(prop_test, broom::tidy)
  ) |> 
  select(prop_test_tidy) |> 
  unnest(prop_test_tidy) |> 
  select(estimate, starts_with("conf"))

baltimore_prop
```

    ## # A tibble: 1 × 3
    ##   estimate conf.low conf.high
    ##      <dbl>    <dbl>     <dbl>
    ## 1    0.646    0.628     0.663

``` r
wp_prop |> 
  mutate(
    prop_test = map2(open_homicides, total_homicides, prop.test),
    prop_test_tidy = map(prop_test, broom::tidy)
  ) |> 
  unnest(prop_test_tidy) |> 
  select(city_state, estimate, starts_with("conf")) |> 
  arrange(desc(estimate)) |> 
  mutate(
    city_state = fct_reorder(city_state, estimate)
  ) |> 
  ggplot(aes(x = city_state, y = estimate, fill = city_state)) +
    geom_col() + 
    geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
    viridis::scale_fill_viridis(discrete = TRUE) +
    labs(
      x = "City, state",
      y = "Estimated proportion of unsolved homicides"
    ) +
    theme(
      axis.text.x = element_text(angle = 90, vjust = 0.5) 
    ) +
    guides(
      fill = "none"
    )
```

    ## Warning: There was 1 warning in `mutate()`.
    ## ℹ In argument: `prop_test = map2(open_homicides, total_homicides, prop.test)`.
    ## Caused by warning in `.f()`:
    ## ! Chi-squared approximation may be incorrect

![](p8105_hw5_jt3466_files/figure-gfm/q3%20all%20cities-1.png)<!-- -->

There is a super wide CI for Tulsa, AL, which makes sense given that
there was 1 total homicide and it was a closed case, leading to low
confidence for the estimated proportion and a wide CI.
